Normalization as suggested in Project
def normalize_query(query):
    query_l = query.lower()
    query_s = re.sub(r'[^a-zA-Z0-9]+', ' ', query_l)
    query_norm = ' '.join([stemmer.stem(token) for token in query_s.split(' ')])
    return query_norm

Min Queries per Category = 100
> FastText Parameters: Default (lr=0.1, epoch=5, wordNgrams=1)
>> avg.loss = 5.263757
>> P@1 = 0.465, P@2 = 0.282, P@3 = 0.205, P@5 = 0.135
>> R@1 = 0.465, R@2 = 0.564, R@3 = 0.616, R@5 = 0.677

> FastText Parameters: lr=0.5, epoch=25, wordNgrams=2
>> avg.loss = 2.389292
>> P@1 = 0.517, P@2 = 0.322, P@3 = 0.234, P@5 = 0.152
>> R@1 = 0.517, R@2 = 0.645, R@3 = 0.702, R@5 = 0.759


Min Queries per Category = 1000
> FastText Parameters: Default (lr=0.1, epoch=5, wordNgrams=1)
>> avg.loss = 3.726431
>> P@1 = 0.497, P@2 = 0.302, P@3 = 0.221, P@5 = 0.146
>> R@1 = 0.497, R@2 = 0.604, R@3 = 0.661, R@5 = 0.728

> FastText Parameters: lr=0.5, epoch=25, wordNgrams=2
>> avg.loss = 2.218524
>> P@1 = 0.526, P@2 = 0.325, P@3 = 0.237, P@5 = 0.155
>> R@1 = 0.526, R@2 = 0.649, R@3 = 0.711, R@5 = 0.777

> FastText Parameters: lr=0.6, epoch=25, wordNgrams=2
>> avg.loss = 2.493556 (Loss did not decrease on increasing lr from 0.5 to 0.6)

> FastText Parameters: lr=0.75, epoch=25, wordNgrams=2
>> avg.loss = 3.038566 (Loss did not decrease on increasing lr from 0.6 to 0.75)

> FastText Parameters: lr=0.4, epoch=25, wordNgrams=2
>> avg.loss = 1.994498
>> P@1 = 0.524, P@2 = 0.325, P@3 = 0.238, P@5 = 0.156
>> R@1 = 0.524, R@2 = 0.649, R@3 = 0.713, R@5 = 0.778